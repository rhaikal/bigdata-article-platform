# Subscriptions logs configuration
agent.sources = r_subs
agent.sinks = k_subs
agent.channels = c_subs

# Interceptors
agent.sources.r_subs.interceptors = i_timestamp i_host
agent.sources.r_subs.interceptors.i_timestamp.type = timestamp
agent.sources.r_subs.interceptors.i_host.type = org.apache.flume.interceptor.HostInterceptor$Builder
agent.sources.r_subs.interceptors.i_host.preserveExisting = true
agent.sources.r_subs.interceptors.i_host.hostHeader = hostname

# Source
agent.sources.r_subs.type = spooldir
agent.sources.r_subs.consumeOrder = oldest
agent.sources.r_subs.spoolDir = /opt/airflow/data/logs/subscriptions
agent.sources.r_subs.fileHeader = false
agent.sources.r_subs.basenameHeader = true
agent.sources.r_subs.ignorePattern = ^(?!.*\.json$).*$
agent.sources.r_subs.deletePolicy = never

# Sink
agent.sinks.k_subs.type = hdfs
agent.sinks.k_subs.hdfs.path = hdfs://hadoop-namenode${HDFS_DATA_DIR:-/data}/raw/logs/subscriptions/%Y%m%d
agent.sinks.k_subs.hdfs.filePrefix = subs-%{hostname}-%{timestamp}
agent.sinks.k_subs.hdfs.fileSuffix = .json
agent.sinks.k_subs.hdfs.fileType = DataStream
agent.sinks.k_subs.hdfs.writeFormat = Text
agent.sinks.k_subs.hdfs.minBlockReplicas = 1
agent.sinks.k_subs.hdfs.callTimeout = 10000
agent.sinks.k_subs.hdfs.ipc.client.connect.timeout = 10000
agent.sinks.k_subs.hdfs.useLocalTimeStamp = true
agent.sinks.k_subs.hdfs.batchSize = 
agent.sinks.k_subs.hdfs.rollInterval = 5
agent.sinks.k_subs.hdfs.rollSize = 0
agent.sinks.k_subs.hdfs.rollCount = 0

# Channel
agent.channels.c_subs.type = memory
agent.channels.c_subs.capacity = 10000
agent.channels.c_subs.transactionCapacity = 100

# Binding
agent.sources.r_subs.channels = c_subs
agent.sinks.k_subs.channel = c_subs