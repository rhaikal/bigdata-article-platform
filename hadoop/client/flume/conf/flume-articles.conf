# Articles logs configuration
agent.sources = r_articles
agent.sinks = k_articles
agent.channels = c_articles

# Interceptors
agent.sources.r_articles.interceptors = i_timestamp i_host
agent.sources.r_articles.interceptors.i_timestamp.type = timestamp
agent.sources.r_articles.interceptors.i_host.type = org.apache.flume.interceptor.HostInterceptor$Builder
agent.sources.r_articles.interceptors.i_host.preserveExisting = true
agent.sources.r_articles.interceptors.i_host.hostHeader = hostname

# Source
agent.sources.r_articles.type = spooldir
agent.sources.r_articles.consumeOrder = oldest
agent.sources.r_articles.spoolDir = /opt/airflow/data/logs/articles
agent.sources.r_articles.fileHeader = false
agent.sources.r_articles.basenameHeader = true
agent.sources.r_articles.ignorePattern = ^(?!.*\.json$).*$
agent.sources.r_articles.deletePolicy = never

# Sink
agent.sinks.k_articles.type = hdfs
agent.sinks.k_articles.hdfs.path = hdfs://hadoop-namenode${HDFS_DATA_DIR:-/data}/raw/logs/articles/%Y%m%d
agent.sinks.k_articles.hdfs.filePrefix = articles-%{hostname}-%{timestamp}
agent.sinks.k_articles.hdfs.fileSuffix = .json
agent.sinks.k_articles.hdfs.fileType = DataStream
agent.sinks.k_articles.hdfs.writeFormat = Text
agent.sinks.k_articles.hdfs.minBlockReplicas = 1
agent.sinks.k_articles.hdfs.callTimeout = 10000
agent.sinks.k_articles.hdfs.ipc.client.connect.timeout = 10000
agent.sinks.k_articles.hdfs.useLocalTimeStamp = true
agent.sinks.k_articles.hdfs.batchSize = 
agent.sinks.k_articles.hdfs.rollInterval = 5
agent.sinks.k_articles.hdfs.rollSize = 0
agent.sinks.k_articles.hdfs.rollCount = 0

# Channel
agent.channels.c_articles.type = memory
agent.channels.c_articles.capacity = 10000
agent.channels.c_articles.transactionCapacity = 100

# Binding
agent.sources.r_articles.channels = c_articles
agent.sinks.k_articles.channel = c_articles