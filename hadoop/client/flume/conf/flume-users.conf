# Users logs configuration
agent.sources = r_users
agent.sinks = k_users
agent.channels = c_users

# Interceptors
agent.sources.r_users.interceptors = i_timestamp i_host
agent.sources.r_users.interceptors.i_timestamp.type = timestamp
agent.sources.r_users.interceptors.i_host.type = org.apache.flume.interceptor.HostInterceptor$Builder
agent.sources.r_users.interceptors.i_host.preserveExisting = true
agent.sources.r_users.interceptors.i_host.hostHeader = hostname

# Source
agent.sources.r_users.type = spooldir
agent.sources.r_users.consumeOrder = oldest
agent.sources.r_users.spoolDir = /opt/airflow/data/logs/users
agent.sources.r_users.fileHeader = false
agent.sources.r_users.basenameHeader = true
agent.sources.r_users.ignorePattern = ^(?!.*\.json$).*$
agent.sources.r_users.deletePolicy = never

# Sink
agent.sinks.k_users.type = hdfs
agent.sinks.k_users.hdfs.path = hdfs://hadoop-namenode${HDFS_DATA_DIR:-/data}/raw/logs/users/%Y%m%d
agent.sinks.k_users.hdfs.filePrefix = users-%{hostname}-%{timestamp}
agent.sinks.k_users.hdfs.fileSuffix = .json
agent.sinks.k_users.hdfs.fileType = DataStream
agent.sinks.k_users.hdfs.writeFormat = Text
agent.sinks.k_users.hdfs.minBlockReplicas = 1
agent.sinks.k_users.hdfs.callTimeout = 10000
agent.sinks.k_users.hdfs.ipc.client.connect.timeout = 10000
agent.sinks.k_users.hdfs.useLocalTimeStamp = true
agent.sinks.k_users.hdfs.batchSize = 15
agent.sinks.k_users.hdfs.rollInterval = 5
agent.sinks.k_users.hdfs.rollSize = 0
agent.sinks.k_users.hdfs.rollCount = 0

# Channel
agent.channels.c_users.type = memory
agent.channels.c_users.capacity = 10000
agent.channels.c_users.transactionCapacity = 100

# Binding
agent.sources.r_users.channels = c_users
agent.sinks.k_users.channel = c_users